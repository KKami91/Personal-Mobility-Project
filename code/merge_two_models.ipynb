{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, box_iou, \\\n",
    "    non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, clip_coords, set_logging, increment_path\n",
    "from utils.loss import compute_loss\n",
    "from utils.metrics import ap_per_class\n",
    "from utils.plots import plot_images, output_to_target\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_helmet = argparse.ArgumentParser(prog='test.py')\n",
    "parser_helmet.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
    "parser_helmet.add_argument('--aug', type=str, default='n', help='in test py img augmentation')\n",
    "parser_helmet.add_argument('--mode', type=str, default='helmet', help='which label to train with (both/helmet/alone)')\n",
    "opt_helmet = parser_helmet.parse_args(args=[])\n",
    "\n",
    "parser_alone = argparse.ArgumentParser(prog='test.py')\n",
    "parser_alone.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
    "parser_alone.add_argument('--aug', type=str, default='n', help='in test py img augmentation')\n",
    "parser_alone.add_argument('--mode', type=str, default='alone', help='which label to train with (both/helmet/alone)')\n",
    "opt_alone = parser_alone.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_helmet = 'data/helmet.yaml'\n",
    "data_alone = 'data/alone.yaml'\n",
    "weights_helmet = 'runs/train/helmet/weights/best_ap50.pt'\n",
    "weights_alone = 'runs/train/alone/weights/best_ap50.pt'\n",
    "\n",
    "aug = 'n'\n",
    "batch_size = 32\n",
    "imgsz = 512\n",
    "augment = False\n",
    "verbose=False\n",
    "model=None\n",
    "dataloader=None\n",
    "save_dir=Path('')\n",
    "save_txt=False\n",
    "save_conf=False\n",
    "plots=True\n",
    "log_imgs=0\n",
    "device= 'cpu'\n",
    "\n",
    "task = 'val' #todo, ['val','test']\n",
    "conf_thres = 0.001 #todo\n",
    "iou_thres = 0.65 #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using torch 1.10.0+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 751 layers, 150960248 parameters, 0 gradients, 232.2 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 751 layers, 150960248 parameters, 0 gradients, 232.2 GFLOPS\n",
      "Scanning labels /opt/ml/final_project/data/val/labels_helmet.cache3 (245 found, 0 missing, 0 empty, 0 duplicate, for 245 images): 245it [00:00, 12111.55it/s]\n",
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "set_logging()\n",
    "device = select_device(device, batch_size=batch_size)\n",
    "\n",
    "save_dir = '/opt/ml/final_project/final-project-level3-cv-01/code/runs/merge_test'\n",
    "model_helmet = attempt_load(weights_helmet, map_location=device)\n",
    "model_alone = attempt_load(weights_alone, map_location=device)\n",
    "imgsz = check_img_size(imgsz,s=model_helmet.stride.max())\n",
    "\n",
    "model_helmet.eval()\n",
    "model_alone.eval()\n",
    "\n",
    "with open(data_helmet) as f:\n",
    "    data_helmet = yaml.load(f,Loader=yaml.FullLoader)\n",
    "check_dataset(data_helmet)\n",
    "\n",
    "with open(data_alone) as f:\n",
    "    data_alone = yaml.load(f,Loader=yaml.FullLoader)\n",
    "check_dataset(data_alone)\n",
    "\n",
    "img = torch.zeros((1, 3, imgsz, imgsz), device=device)\n",
    "_ = model_helmet(img)  # run once\n",
    "_ = model_alone(img)  # run once\n",
    "\n",
    "path_helmet = data_helmet['test'] if task == 'test' else data_helmet['val']  # path to val/test images\n",
    "path_alone = data_alone['test'] if task == 'test' else data_alone['val']  # path to val/test images\n",
    "\n",
    "dataloader = create_dataloader(path_helmet, imgsz, batch_size, model_helmet.stride.max(), opt_helmet, pad=0.5, rect=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "\n",
    "def remove_overlap(iou, priority = 'alone'):\n",
    "    if priority == 'alone':\n",
    "        # 가로 먼저 max\n",
    "        iou *= (iou == iou.max(dim=1, keepdim=True)[0])\n",
    "        iou *= (iou == iou.max(dim=0, keepdim=True)[0])\n",
    "    else :\n",
    "        iou *= (iou == iou.max(dim=1, keepdim=True)[0])\n",
    "        iou *= (iou == iou.max(dim=0, keepdim=True)[0])\n",
    "    \n",
    "    return iou\n",
    "        \n",
    "def both_class(helmet, alone):\n",
    "    return 2*alone + helmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:45<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# targets [n,6] - (img_num,cls,x,y,w,h)\n",
    "# paths [32,1]\n",
    "# img [nb,3,w,h]\n",
    "for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader)):\n",
    "    img = img.to(device, non_blocking=True)\n",
    "    img = img.float()\n",
    "    img /= 255.0\n",
    "    targets.to(device)\n",
    "    \n",
    "    nb, _, height, width = img.shape\n",
    "    whwh = torch.Tensor([width,height, width, height]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #run model - [32,11475,7] (x,y,w,h,obj_conf,cls_conf for each class)\n",
    "        inf_out_helmet, train_out_helmet = model_helmet(img, augment=augment)\n",
    "        inf_out_alone, train_out_alone = model_alone(img, augment=augment)\n",
    "    \n",
    "        #run nms - [32,-,6] (x1,y1,x2,y2,conf,cls)\n",
    "        output_helmet = non_max_suppression(inf_out_helmet, conf_thres=conf_thres, iou_thres=iou_thres, merge=False, classes=None, agnostic=True)\n",
    "        output_alone = non_max_suppression(inf_out_alone, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "\n",
    "    output = []\n",
    "    # ith image in a batch\n",
    "    for image_i in range(nb):\n",
    "        # prediction for ith image (x1,y1,x2,y2,conf,cls)\n",
    "        pred_helmet = output_helmet[image_i]\n",
    "        pred_alone = output_alone[image_i]\n",
    "        \n",
    "        # helmet or alone에 box 없는 경우\n",
    "        if len(pred_helmet) == 0 or len(pred_alone) == 0 :\n",
    "            continue\n",
    "        \n",
    "\n",
    "        #pred_both\n",
    "        iou = box_iou(pred_helmet[:,:4],pred_alone[:,:4])\n",
    "        iou = remove_overlap(iou)\n",
    "        i,j = (iou > iou_thres).nonzero(as_tuple=False).T\n",
    "\n",
    "        pred_helmet = pred_helmet[i]\n",
    "        pred_alone = pred_alone[j]\n",
    "\n",
    "        box_n = len(pred_helmet)\n",
    "        pred_both = torch.zeros_like(pred_helmet)\n",
    "\n",
    "        output.append(pred_both)\n",
    "        \n",
    "\n",
    "        for index in range(box_n):\n",
    "            # todo - box merge는 나중에 & obj_conf로 하는 것이 더 정확\n",
    "            if pred_helmet[index][4] > pred_alone[index][4]:\n",
    "                pred_both[index][:4] = pred_helmet[index][:4]\n",
    "            else:\n",
    "                pred_both[index][:4] = pred_alone[index][:4]\n",
    "            pred_both[index][4] = pred_helmet[index][4] * pred_alone[index][4]\n",
    "            pred_both[index][5] = both_class(pred_helmet[index][5],pred_alone[index][5])\n",
    "        \n",
    "        path = Path(paths[image_i])\n",
    "        \n",
    "\n",
    "\n",
    "        # (cls,x,y,w,h)\n",
    "        labels = targets[targets[:,0]==image_i, 1:]\n",
    "        gn = torch.tensor(shapes[image_i][0])[[1,0,1,0]]\n",
    "        x = pred_both.clone()\n",
    "        x[:,:4] = scale_coords(img[image_i].shape[1:],x[:,:4],shapes[image_i][0],shapes[image_i][1])\n",
    "        # for *xyxy, conf, cls in x:\n",
    "        #     xywh = (xyxy2xywh(torch.tensor(xyxy).view(1,4))/gn).view(-1).tolist()\n",
    "        #     line = (cls, *xywh, conf)\n",
    "        #     with open(save_dir / 'labels' / (path.stem+'.txt'),'a') as f:\n",
    "        #         f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "        break\n",
    "    break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[6.13896e+01, 7.66511e+01, 1.39537e+02, 2.10082e+02, 9.47482e-01, 1.00000e+00],\n",
       "         [3.89434e+02, 6.74094e+01, 5.07789e+02, 2.07215e+02, 3.01251e-02, 1.00000e+00],\n",
       "         [2.77112e+02, 5.97449e+01, 2.94687e+02, 1.12453e+02, 4.62524e-03, 1.00000e+00]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Non-Maximum Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = inf_out_helmet\n",
    "conf_thres=0.1\n",
    "iou_thres=0.6\n",
    "merge=False\n",
    "classes=None\n",
    "agnostic=False\n",
    "\n",
    "nc = prediction[0].shape[1]-5\n",
    "xc = prediction[...,4]>0.1\n",
    "\n",
    "min_wh, max_wh =2,4096\n",
    "max_det = 300\n",
    "time_limit = 10.0\n",
    "redundant = True\n",
    "multi_label = False #확인필요\n",
    "output = [torch.zeros(0,6)]*prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11475, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x => [x,y,w,h,conf,cls1,cls2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, x in enumerate(prediction):\n",
    "    x = x[xc[xi]]    \n",
    "    x[:,5:] *= x[:,4:5]\n",
    "    box = xywh2xyxy(x[:,:4])    \n",
    "\n",
    "    i,j = ((x[:,5:] > conf_thres).nonzero(as_tuple=False).T)\n",
    "    x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "   \n",
    "    c = x[:,5:6] * (0 if agnostic else max_wh)\n",
    "    boxes, scores = x[:,:4] + c, x[:,4]\n",
    "    \n",
    "    i = torch.ops.torchvision.nms(boxes,scores,iou_thres)\n",
    "\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 4])\n"
     ]
    }
   ],
   "source": [
    "print(boxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 36,  1], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = box_iou(boxes[i],boxes) > iou_thres\n",
    "weights = iou * scores # 겹치는 애들의 score만 담겨있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.96911, 0.31675, 0.00000, 0.00000, 0.96161, 0.25876, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.97065, 0.32645, 0.00000, 0.00000, 0.96185, 0.26204, 0.75962, 0.95285, 0.00000, 0.00000, 0.23226, 0.95426, 0.00000, 0.00000, 0.76906,\n",
       "         0.95700, 0.00000, 0.00000, 0.25917, 0.95554, 0.00000, 0.00000, 0.75373, 0.93551, 0.00000, 0.00000, 0.20580, 0.93377, 0.00000, 0.00000, 0.70325, 0.00000, 0.00000, 0.89013, 0.89927, 0.00000, 0.00000, 0.38096, 0.58364, 0.00000, 0.00000, 0.76206, 0.79237, 0.00000, 0.00000, 0.22858, 0.00000, 0.00000, 0.00000,\n",
       "         0.00000],\n",
       "        [0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.45703, 0.66789, 0.00000, 0.00000, 0.64723, 0.68575, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.45689, 0.69521, 0.00000, 0.00000, 0.65722, 0.70071, 0.00000, 0.00000, 0.00000, 0.00000, 0.92003, 0.94481, 0.00000, 0.00000, 0.31033, 0.61730, 0.00000,\n",
       "         0.00000, 0.92084, 0.94831, 0.00000, 0.00000, 0.30400, 0.62848, 0.00000, 0.00000, 0.91380, 0.94827, 0.00000, 0.00000, 0.22346, 0.57438, 0.00000, 0.49381, 0.51089, 0.00000, 0.00000, 0.87431, 0.87382, 0.00000, 0.00000, 0.45009, 0.50746, 0.00000, 0.00000, 0.88739, 0.89046, 0.00000, 0.19197, 0.28997, 0.72745,\n",
       "         0.73995],\n",
       "        [0.60798, 0.76104, 0.43003, 0.68035, 0.75585, 0.47043, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.63401, 0.74148, 0.46366, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "         0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000,\n",
       "         0.00000]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[276.80698,  59.43233, 294.75516, 112.06213],\n",
       "        [277.11203,  59.74487, 294.68680, 112.45279],\n",
       "        [276.62393,  59.90397, 294.43344, 112.36182],\n",
       "        [277.20520,  59.71339, 294.88123, 112.11556],\n",
       "        [277.27734,  59.95576, 294.67615, 111.97173],\n",
       "        [276.88263,  60.30616, 294.48621, 112.68191],\n",
       "        [386.30835,  67.42755, 505.37646, 206.13791],\n",
       "        [392.62170,  66.76973, 506.28363, 205.94713],\n",
       "        [ 60.61163,  77.43890, 139.28905, 210.87970],\n",
       "        [ 61.04763,  77.34317, 140.26407, 209.71539],\n",
       "        [387.94220,  65.83866, 505.37506, 210.02353],\n",
       "        [391.16357,  65.17422, 506.88971, 209.14368],\n",
       "        [ 60.84024,  77.69925, 138.88235, 210.05832],\n",
       "        [ 60.98352,  78.15411, 140.14357, 210.83667],\n",
       "        [277.08905,  59.85069, 294.90881, 112.04035],\n",
       "        [277.27585,  60.20796, 294.73459, 112.41620],\n",
       "        [276.75940,  60.25511, 294.81714, 112.24501],\n",
       "        [386.35049,  68.29788, 505.68698, 205.17816],\n",
       "        [393.13177,  67.03396, 506.14178, 205.16415],\n",
       "        [ 60.59962,  77.44530, 139.52696, 210.88065],\n",
       "        [ 61.15373,  77.43037, 139.96948, 209.98293],\n",
       "        [388.05643,  67.70785, 505.36234, 208.27377],\n",
       "        [391.68390,  66.37134, 506.83911, 207.74884],\n",
       "        [ 60.94736,  77.62328, 139.26379, 210.38449],\n",
       "        [ 61.00637,  77.91703, 140.06050, 210.55301],\n",
       "        [ 61.10401,  77.87426, 139.04343, 209.88574],\n",
       "        [ 60.94409,  78.42097, 139.38759, 210.12329],\n",
       "        [390.00146,  66.78445, 507.05548, 206.77423],\n",
       "        [389.32587,  67.02534, 508.11499, 206.96660],\n",
       "        [ 61.56672,  78.49708, 139.45099, 209.28455],\n",
       "        [ 60.84576,  78.60599, 139.24628, 209.24655],\n",
       "        [389.15860,  66.59077, 505.86777, 209.02432],\n",
       "        [391.10153,  66.67709, 509.74863, 207.75153],\n",
       "        [ 61.15725,  77.36979, 138.94707, 210.14368],\n",
       "        [ 60.90701,  78.25249, 139.32962, 210.07693],\n",
       "        [389.93665,  67.23761, 506.89069, 207.34204],\n",
       "        [389.43390,  67.40947, 507.78864, 207.21501],\n",
       "        [ 61.66347,  78.36801, 139.14441, 209.43741],\n",
       "        [ 61.07308,  78.28127, 139.25287, 209.49541],\n",
       "        [388.90027,  66.73852, 506.18146, 208.68600],\n",
       "        [390.95633,  66.91900, 509.20737, 207.24481],\n",
       "        [ 61.04057,  77.64695, 138.81485, 210.18753],\n",
       "        [ 60.48047,  78.09555, 139.61702, 209.90338],\n",
       "        [391.11496,  67.03124, 506.58182, 206.49976],\n",
       "        [389.18271,  67.09805, 508.27023, 206.52747],\n",
       "        [ 61.71309,  78.64415, 138.85223, 209.38362],\n",
       "        [ 60.76122,  78.27370, 139.29410, 209.11841],\n",
       "        [390.15497,  67.04935, 504.87952, 208.89316],\n",
       "        [392.37964,  66.64282, 509.10529, 207.79834],\n",
       "        [ 60.07402,  76.47993, 140.45018, 208.33833],\n",
       "        [389.90536,  68.17157, 505.26102, 205.32724],\n",
       "        [390.84344,  68.90549, 509.26746, 205.55823],\n",
       "        [ 61.33548,  77.58457, 139.40993, 211.25189],\n",
       "        [ 61.25579,  77.81181, 139.22417, 210.13950],\n",
       "        [391.95111,  65.73937, 507.07495, 208.69995],\n",
       "        [389.22858,  65.50584, 508.54413, 208.08128],\n",
       "        [ 60.72750,  78.51772, 140.15210, 211.81197],\n",
       "        [ 60.36425,  76.18219, 139.59415, 208.39764],\n",
       "        [388.51031,  68.24033, 505.89227, 205.37393],\n",
       "        [390.46091,  67.60149, 509.39957, 207.39038],\n",
       "        [ 61.10055,  77.60367, 139.70026, 212.20834],\n",
       "        [ 61.01970,  77.57392, 139.07225, 211.15729],\n",
       "        [391.31726,  67.15942, 506.60120, 207.78439],\n",
       "        [388.83835,  66.78135, 508.34396, 207.31448],\n",
       "        [ 60.73526,  78.17395, 140.29526, 212.53668],\n",
       "        [389.22629,  66.09297, 505.17795, 207.32906],\n",
       "        [390.64642,  65.88846, 508.66577, 208.65857],\n",
       "        [391.21686,  65.04234, 507.88330, 210.12184],\n",
       "        [389.54581,  64.20219, 508.87912, 209.98564]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[i,:4] = torch.mm(weights, x[:,:4]) / weights.sum(1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False, False, False,  True,  True, False, False,  True,  True, False, False, False, False, False,  True,  True, False, False,  True,  True,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False,\n",
      "         False,  True,  True, False, False,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True, False, False, False, False],\n",
      "        [False, False, False, False, False, False,  True,  True, False, False,  True,  True, False, False, False, False, False,  True,  True, False, False,  True,  True, False, False, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,\n",
      "          True, False, False,  True,  True, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False, False,  True,  True, False,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True, False, False, False, False, False, False, False, False,  True,  True,  True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]], device='cuda:0')\n",
      "tensor([28, 32,  9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(iou)\n",
    "print(iou.sum(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightweight",
   "language": "python",
   "name": "lightweight"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
